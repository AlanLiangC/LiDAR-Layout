model:
  base_learning_rate: 1.0e-06
  target: lidm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0205
    num_timesteps_cond: 1
    log_every_t: 100
    timesteps: 1000
    image_size: [8, 128]
    channels: 8
    monitor: val/loss_simple_ema
    first_stage_key: image
    # condition
    cond_stage_key: layout
    conditioning_key: 'layout_crossattn'
    cond_stage_trainable: true
    verbose: false

    unet_config:
      target: lidm.modules.unets.object_cross_unet.LayoutDiffusionUNetModel
      params:
        image_size: [8, 128]
        use_fp16: false
        use_scale_shift_norm: true
        in_channels: 8
        out_channels: 8
        model_channels: 256
        encoder_channels: 256 # assert same as layout_encoder.hidden_dim
        num_head_channels: 64
        num_heads: -1
        num_heads_upsample: -1
        num_res_blocks: 2
        num_attention_blocks: 1
        resblock_updown: True
        attention_ds: [ 8,4,2 ] # attetion_resolution: 32,16,8
        channel_mult: [ 1,2,4 ]
        dropout: 0.1
        use_checkpoint: False
        use_positional_embedding_for_attention: True
        attention_block_type: 'ObjectAwareCrossAttention'

    first_stage_config:
      target: lidm.models.autoencoder.VQModelInterface
      params:
        embed_dim: 8
        n_embed: 16384
        lib_name: lidm
        use_mask: False  # False
        ckpt_path: ../models/first_stage_models/nusc/f_c2_p4/last.ckpt
        ddconfig:
          double_z: false
          z_channels: 8
          in_channels: 1
          out_ch: 1
          ch: 64
          ch_mult: [1,2,2,4]
          strides: [[1,2],[2,2],[2,2]]
          num_res_blocks: 2
          attn_levels: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity
    cond_stage_config:
      target: lidm.modules.encoders.layout_encoder.LayoutTransformerEncoder
      params:
        feature_map_size: [8, 128]
        used_condition_types: [
          'obj_class', 'obj_bbox', 'is_valid_obj'
        ]
        layout_length: 12
        num_classes_for_layout_object: 8
        mask_size_for_layout_object: 32
        hidden_dim: 256
        output_dim: 1024 # model_channels x 4
        num_layers: 6
        num_heads: 8
        use_final_ln: True
        use_positional_embedding: False
        not_use_layout_fusion_module: true
        resolution_to_attention: [ 4, 2, 1 ]
        use_key_padding_mask: False

data:
  target: train_lidm.DataModuleFromConfig
  params:
    batch_size: 16
    num_workers: 8
    wrap: true
    use_collate_fn: true
    dataset:
      size: [32, 1024]
      fov: [ 10,-30 ]
      depth_range: [ 1.0,56.0 ]
      depth_scale: 5.84  # np.log2(depth_max + 1)
      log_scale: true
      x_range: [ -50.0, 50.0 ]
      y_range: [ -50.0, 50.0 ]
      z_range: [ -4.0, 2.0 ]
      resolution: 1
      num_channels: 1
      num_cats: 10
      num_views: 2
      num_sem_cats: 19
      filtered_map_cats: [ ]
    aug:
      rotate: false
      flip: false
      flip_w_box: true
      rotate_w_box: true
      keypoint_drop: false
      keypoint_drop_range: [ 5,20 ]
      randaug: false
    train:
      target: lidm.data.nusc_dataset.nuScenesLayoutBase
      params:
        condition_key: layout
        data_root: /home/alan/AlanLiang/Dataset/Nuscenes/v1.0-trainval
        # data_root: /data1/liangao/Dataset/openpcd_nuscenes
        info_path: /home/alan/AlanLiang/Projects/AlanLiang/CentralScene/data/nuscenes/nuscenes_infos_train.pkl
    validation:
      target: lidm.data.nusc_dataset.nuScenesLayoutBase
      params:
        condition_key: layout
        data_root: /home/alan/AlanLiang/Dataset/Nuscenes/v1.0-trainval
        # data_root: /data1/liangao/Dataset/openpcd_nuscenes
        info_path: /home/alan/AlanLiang/Projects/AlanLiang/CentralScene/data/nuscenes/nuscenes_infos_val.pkl

lightning:
  callbacks:
    image_logger:
      target: train_lidm.ImageLogger
      params:
        batch_frequency: 5000
        max_images: 8
        increase_log_steps: False

  trainer:
    benchmark: true